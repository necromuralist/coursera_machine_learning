{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "# Training Logistic Regression via Stochastic Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The goal of this notebook is to implement a logistic regression classifier using stochastic gradient ascent. You will:\n",
    "\n",
    " * Extract features from Amazon product reviews.\n",
    " * Convert an SFrame into a NumPy array.\n",
    " * Write a function to compute the derivative of log likelihood function with respect to a single coefficient.\n",
    " * Implement stochastic gradient ascent.\n",
    " * Compare convergence of stochastic gradient ascent with that of batch gradient ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "# Fire up GraphLab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Make sure you have the latest version of GraphLab Create. Upgrade by\n",
    "\n",
    "```\n",
    "   pip install graphlab-create --upgrade\n",
    "```\n",
    "See [this page](https://dato.com/download/) for detailed instructions on upgrading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A newer version of GraphLab Create (v1.9) is available! Your current version is v1.8.5.\n",
      "\n",
      "You can use pip to upgrade the graphlab-create package. For more information see https://dato.com/products/create/upgrade.\n"
     ]
    }
   ],
   "source": [
    "# python standard library\n",
    "from __future__ import division\n",
    "import os\n",
    "\n",
    "# third-party\n",
    "import graphlab\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Load and process review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "For this assignment, we will use the same subset of the Amazon product review dataset that we used in Module 3 assignment. The subset was chosen to contain similar numbers of positive and negative reviews, as the original dataset consisted of mostly positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create is assigned to rsnakamura@acm.org and will expire on March 24, 2017. For commercial licensing options, visit https://dato.com/buy/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-04-30 13:09:38,310 [INFO] graphlab.cython.cy_server, 176: GraphLab Create v1.8.5 started. Logging: /tmp/graphlab_server_1462046976.log\n"
     ]
    }
   ],
   "source": [
    "products = graphlab.SFrame('../large_data/amazon_baby_subset.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Just like we did previously, we will work with a hand-curated list of important words extracted from the review data. We will also perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string manipulation functionality.\n",
    "2. Compute word counts (only for the important_words)\n",
    "\n",
    "Refer to Module 3 assignment for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../large_data/important_words.json', 'r') as f: \n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]\n",
    "\n",
    "# Remote punctuation\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
    "\n",
    "# Split out the words into individual columns\n",
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The SFrame **products** now contains one column for each of the 193 **important_words**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns:\n\tname\tstr\n\treview\tstr\n\trating\tfloat\n\tsentiment\tint\n\treview_clean\tstr\n\tbaby\tint\n\tone\tint\n\tgreat\tint\n\tlove\tint\n\tuse\tint\n\twould\tint\n\tlike\tint\n\teasy\tint\n\tlittle\tint\n\tseat\tint\n\told\tint\n\twell\tint\n\tget\tint\n\talso\tint\n\treally\tint\n\tson\tint\n\ttime\tint\n\tbought\tint\n\tproduct\tint\n\tgood\tint\n\tdaughter\tint\n\tmuch\tint\n\tloves\tint\n\tstroller\tint\n\tput\tint\n\tmonths\tint\n\tcar\tint\n\tstill\tint\n\tback\tint\n\tused\tint\n\trecommend\tint\n\tfirst\tint\n\teven\tint\n\tperfect\tint\n\tnice\tint\n\tbag\tint\n\ttwo\tint\n\tusing\tint\n\tgot\tint\n\tfit\tint\n\taround\tint\n\tdiaper\tint\n\tenough\tint\n\tmonth\tint\n\tprice\tint\n\tgo\tint\n\tcould\tint\n\tsoft\tint\n\tsince\tint\n\tbuy\tint\n\troom\tint\n\tworks\tint\n\tmade\tint\n\tchild\tint\n\tkeep\tint\n\tsize\tint\n\tsmall\tint\n\tneed\tint\n\tyear\tint\n\tbig\tint\n\tmake\tint\n\ttake\tint\n\teasily\tint\n\tthink\tint\n\tcrib\tint\n\tclean\tint\n\tway\tint\n\tquality\tint\n\tthing\tint\n\tbetter\tint\n\twithout\tint\n\tset\tint\n\tnew\tint\n\tevery\tint\n\tcute\tint\n\tbest\tint\n\tbottles\tint\n\twork\tint\n\tpurchased\tint\n\tright\tint\n\tlot\tint\n\tside\tint\n\thappy\tint\n\tcomfortable\tint\n\ttoy\tint\n\table\tint\n\tkids\tint\n\tbit\tint\n\tnight\tint\n\tlong\tint\n\tfits\tint\n\tsee\tint\n\tus\tint\n\tanother\tint\n\tplay\tint\n\tday\tint\n\tmoney\tint\n\tmonitor\tint\n\ttried\tint\n\tthought\tint\n\tnever\tint\n\titem\tint\n\thard\tint\n\tplastic\tint\n\thowever\tint\n\tdisappointed\tint\n\treviews\tint\n\tsomething\tint\n\tgoing\tint\n\tpump\tint\n\tbottle\tint\n\tcup\tint\n\twaste\tint\n\treturn\tint\n\tamazon\tint\n\tdifferent\tint\n\ttop\tint\n\twant\tint\n\tproblem\tint\n\tknow\tint\n\twater\tint\n\ttry\tint\n\treceived\tint\n\tsure\tint\n\ttimes\tint\n\tchair\tint\n\tfind\tint\n\thold\tint\n\tgate\tint\n\topen\tint\n\tbottom\tint\n\taway\tint\n\tactually\tint\n\tcheap\tint\n\tworked\tint\n\tgetting\tint\n\tordered\tint\n\tcame\tint\n\tmilk\tint\n\tbad\tint\n\tpart\tint\n\tworth\tint\n\tfound\tint\n\tcover\tint\n\tmany\tint\n\tdesign\tint\n\tlooking\tint\n\tweeks\tint\n\tsay\tint\n\twanted\tint\n\tlook\tint\n\tplace\tint\n\tpurchase\tint\n\tlooks\tint\n\tsecond\tint\n\tpiece\tint\n\tbox\tint\n\tpretty\tint\n\ttrying\tint\n\tdifficult\tint\n\ttogether\tint\n\tthough\tint\n\tgive\tint\n\tstarted\tint\n\tanything\tint\n\tlast\tint\n\tcompany\tint\n\tcome\tint\n\treturned\tint\n\tmaybe\tint\n\ttook\tint\n\tbroke\tint\n\tmakes\tint\n\tstay\tint\n\tinstead\tint\n\tidea\tint\n\thead\tint\n\tsaid\tint\n\tless\tint\n\twent\tint\n\tworking\tint\n\thigh\tint\n\tunit\tint\n\tseems\tint\n\tpicture\tint\n\tcompletely\tint\n\twish\tint\n\tbuying\tint\n\tbabies\tint\n\twon\tint\n\ttub\tint\n\talmost\tint\n\teither\tint\n\nRows: 53072\n\nData:\n+-------------------------------+-------------------------------+--------+-----------+\n|              name             |             review            | rating | sentiment |\n+-------------------------------+-------------------------------+--------+-----------+\n| Stop Pacifier Sucking with... | All of my kids have cried ... |  5.0   |     1     |\n| Nature's Lullabies Second ... | We wanted to get something... |  5.0   |     1     |\n| Nature's Lullabies Second ... | My daughter had her 1st ba... |  5.0   |     1     |\n|  Lamaze Peekaboo, I Love You  | One of baby's first and fa... |  4.0   |     1     |\n| SoftPlay Peek-A-Boo Where'... | Very cute interactive book... |  5.0   |     1     |\n|   Our Baby Girl Memory Book   | Beautiful book, I love it ... |  5.0   |     1     |\n| Hunnt&reg; Falling Flowers... | Try this out for a spring ... |  5.0   |     1     |\n| Blessed By Pope Benedict X... | very nice Divine Mercy Pen... |  5.0   |     1     |\n| Cloth Diaper Pins Stainles... | We bought the pins as my 6... |  4.0   |     1     |\n| Cloth Diaper Pins Stainles... | It has been many years sin... |  5.0   |     1     |\n+-------------------------------+-------------------------------+--------+-----------+\n+-------------------------------+------+-----+-------+------+-----+-------+------+\n|          review_clean         | baby | one | great | love | use | would | like |\n+-------------------------------+------+-----+-------+------+-----+-------+------+\n| All of my kids have cried ... |  0   |  0  |   1   |  0   |  0  |   0   |  0   |\n| We wanted to get something... |  0   |  0  |   0   |  0   |  0  |   0   |  0   |\n| My daughter had her 1st ba... |  1   |  0  |   0   |  0   |  0  |   0   |  0   |\n| One of babys first and fav... |  0   |  0  |   0   |  0   |  0  |   0   |  1   |\n| Very cute interactive book... |  0   |  0  |   1   |  0   |  0  |   0   |  0   |\n| Beautiful book I love it t... |  0   |  0  |   1   |  1   |  0  |   0   |  0   |\n| Try this out for a spring ... |  0   |  0  |   0   |  0   |  0  |   0   |  0   |\n| very nice Divine Mercy Pen... |  0   |  0  |   0   |  0   |  0  |   0   |  0   |\n| We bought the pins as my 6... |  0   |  1  |   0   |  0   |  1  |   0   |  0   |\n| It has been many years sin... |  0   |  1  |   0   |  0   |  0  |   0   |  1   |\n+-------------------------------+------+-----+-------+------+-----+-------+------+\n+------+--------+------+-----+------+-----+------+--------+-----+\n| easy | little | seat | old | well | get | also | really | ... |\n+------+--------+------+-----+------+-----+------+--------+-----+\n|  1   |   0    |  0   |  0  |  0   |  0  |  0   |   0    | ... |\n|  0   |   0    |  0   |  0  |  0   |  1  |  0   |   0    | ... |\n|  0   |   0    |  0   |  1  |  0   |  1  |  0   |   0    | ... |\n|  0   |   0    |  0   |  0  |  0   |  0  |  0   |   0    | ... |\n|  0   |   0    |  0   |  0  |  0   |  0  |  0   |   0    | ... |\n|  0   |   0    |  0   |  0  |  0   |  0  |  0   |   0    | ... |\n|  0   |   0    |  0   |  0  |  0   |  0  |  0   |   0    | ... |\n|  0   |   0    |  0   |  0  |  0   |  0  |  0   |   0    | ... |\n|  0   |   0    |  0   |  1  |  0   |  0  |  0   |   0    | ... |\n|  0   |   1    |  0   |  0  |  0   |  1  |  0   |   0    | ... |\n+------+--------+------+-----+------+-----+------+--------+-----+\n[53072 rows x 198 columns]\nNote: Only the head of the SFrame is printed.\nYou can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "We will now split the data into a 90-10 split where 90% is in the training set and 10% is in the validation set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set  : 47780 data points\n",
      "Validation set: 5292 data points\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = products.random_split(.9, seed=1)\n",
    "\n",
    "print 'Training set  : %d data points' % len(train_data)\n",
    "print 'Validation set: %d data points' % len(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Convert SFrame to NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Just like in the earlier assignments, we provide you with a function that extracts columns from an SFrame and converts them into a NumPy array. Two arrays are returned: one representing features and another representing class labels. \n",
    "\n",
    "**Note:** The feature matrix includes an additional column 'intercept' filled with 1's to take account of the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_numpy_data(data_sframe, features, label):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    label_sarray = data_sframe[label]\n",
    "    label_array = label_sarray.to_numpy()\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Note that we convert both the training and validation sets into NumPy arrays.\n",
    "\n",
    "**Warning**: This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "array_data = \"pickles/train_validate_data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "class DataArchive(object):\n",
    "    __slots__ = ()\n",
    "    training_features = 'feature_matrix_train'\n",
    "    training_targets = 'sentiment_train'\n",
    "    validation_features = 'feature_matrix_valid'\n",
    "    validation_targets = 'sentiment_valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(array_data):\n",
    "    feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "    feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment')\n",
    "    archive = {DataArchive.training_features: feature_matrix_train,\n",
    "               DataArchive.training_targets: sentiment_train,\n",
    "               DataArchive.validation_features: feature_matrix_valid,\n",
    "               DataArchive.validation_targets: sentiment_valid}\n",
    "    numpy.savez_compressed(array_data, archive)\n",
    "else:\n",
    "    data_archive = numpy.load(array_data)\n",
    "    feature_matrix_train = data_archive[DataArchive.training_features]\n",
    "    sentiment_train = data_archive[DataArchive.training_targets]\n",
    "    feature_matrix_valid = data_archive[DataArchive.validation_features]\n",
    "    sentiment_valid = data_archive[DataArchive.validation_targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "** Quiz question**: In Module 3 assignment, there were 194 features (an intercept + one feature for each of the 193 important words). In this assignment, we will use stochastic gradient ascent to train the classifier using logistic regression. How does the changing the solver to stochastic gradient ascent affect the number of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Stochastic gradient ascent allows you to use more features, but in this case doesn't change anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Building on logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Let us now build on Module 3 assignment. Recall from lecture that the link function for logistic regression can be defined as:\n",
    "\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ is given by the word counts of **important_words** in the review $\\mathbf{x}_i$. \n",
    "\n",
    "\n",
    "We will use the **same code** as in Module 3 assignment to make probability predictions, since this part is not affected by using stochastic gradient ascent as a solver. Only the way in which the coefficients are learned is affected by using stochastic gradient ascent as a solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    predictions = 1. / (1.+np.exp(-score))    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Derivative of log likelihood with respect to a single coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Let us now work on making minor changes to how the derivative computation is performed for logistic regression.\n",
    "\n",
    "Recall from the lectures and Module 3 assignment that for logistic regression, **the derivative of log likelihood with respect to a single coefficient** is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "In Module 3 assignment, we wrote a function to compute the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts the following two parameters:\n",
    " * `errors` vector containing $(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w}))$ for all $i$\n",
    " * `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$\n",
    " \n",
    "Complete the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    derivative = numpy.dot(errors, feature)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Note**. We are not using regularization in this assignment, but, as discussed in the optional video, stochastic gradient can also be used for regularized logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "To verify the correctness of the gradient computation, we provide a function for computing average log likelihood (which we recall from the last assignment was a topic detailed in an advanced optional video, and used here for its numerical stability).\n",
    "\n",
    "To track the performance of stochastic gradient ascent, we provide a function for computing **average log likelihood**. \n",
    "\n",
    "$$\\ell\\ell_A(\\mathbf{w}) = \\color{red}{\\frac{1}{N}} \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "**Note** that we made one tiny modification to the log likelihood function (called **compute_log_likelihood**) in our earlier assignments. We added a $\\color{red}{1/N}$ term which averages the log likelihood accross all data points. The $\\color{red}{1/N}$ term makes it easier for us to compare stochastic gradient ascent with batch gradient ascent. We will use this function to generate plots that are similar to those you saw in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_avg_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    \n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    logexp = np.log(1. + np.exp(-scores))\n",
    "    \n",
    "    # Simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - logexp)/len(feature_matrix)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "** Quiz Question:** Recall from the lecture and the earlier assignment, the log likelihood (without the averaging term) is given by \n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "How are the functions $\\ell\\ell(\\mathbf{w})$ and $\\ell\\ell_A(\\mathbf{w})$ related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "ll(w)A = ll(w)/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Recall from the lecture that the gradient for a single data point $\\color{red}{\\mathbf{x}_i}$ can be computed using the following formula:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})}{\\partial w_j} = h_j(\\color{red}{\\mathbf{x}_i})\\left(\\mathbf{1}[y_\\color{red}{i} = +1] - P(y_\\color{red}{i} = +1 | \\color{red}{\\mathbf{x}_i}, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "** Computing the gradient for a single data point**\n",
    "\n",
    "Do we really need to re-write all our code to modify $\\partial\\ell(\\mathbf{w})/\\partial w_j$ to $\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})/{\\partial w_j}$? \n",
    "\n",
    "\n",
    "Thankfully **No!**. Using NumPy, we access $\\mathbf{x}_i$ in the training data using `feature_matrix_train[i:i+1,:]`\n",
    "and $y_i$ in the training data using `sentiment_train[i:i+1]`. We can compute $\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})/\\partial w_j$ by re-using **all the code** written in **feature_derivative** and **predict_probability**.\n",
    "\n",
    "\n",
    "We compute $\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})/\\partial w_j$ using the following steps:\n",
    "* First, compute $P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ using the **predict_probability** function with `feature_matrix_train[i:i+1,:]` as the first parameter.\n",
    "* Next, compute $\\mathbf{1}[y_i = +1]$ using `sentiment_train[i:i+1]`.\n",
    "* Finally, call the **feature_derivative** function with `feature_matrix_train[i:i+1, j]` as one of the parameters. \n",
    "\n",
    "Let us follow these steps for `j = 1` and `i = 10`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient single data point: 0.0\n",
      "           --> Should print 0.0\n"
     ]
    }
   ],
   "source": [
    "j = 1                        # Feature number\n",
    "i = 10                       # Data point number\n",
    "coefficients = np.zeros(194) # A point w at which we are computing the gradient.\n",
    "\n",
    "predictions = predict_probability(feature_matrix_train[i:i+1,:], coefficients)\n",
    "indicator = (sentiment_train[i:i+1]==+1)\n",
    "\n",
    "errors = indicator - predictions        \n",
    "gradient_single_data_point = feature_derivative(errors, feature_matrix_train[i:i+1,j])\n",
    "print \"Gradient single data point: %s\" % gradient_single_data_point\n",
    "print \"           --> Should print 0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "** Quiz Question:** The code block above computed $\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})/{\\partial w_j}$ for `j = 1` and `i = 10`.  Is $\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})/{\\partial w_j}$ a scalar or a 194-dimensional vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "It is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Modifying the derivative for using a batch of data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Stochastic gradient estimates the ascent direction using 1 data point, while gradient uses $N$ data points to decide how to update the the parameters.  In an optional video, we discussed the details of a simple change that allows us to use a **mini-batch** of $B \\leq N$ data points to estimate the ascent direction. This simple approach is faster than regular gradient but less noisy than stochastic gradient that uses only 1 data point. Although we encorage you to watch the optional video on the topic to better understand why mini-batches help stochastic gradient, in this assignment, we will simply use this technique, since the approach is very simple and will improve your results.\n",
    "\n",
    "Given a mini-batch (or a set of data points) $\\mathbf{x}_{i}, \\mathbf{x}_{i+1} \\ldots \\mathbf{x}_{i+B}$, the gradient function for this mini-batch of data points is given by:\n",
    "$$\n",
    "\\color{red}{\\sum_{s = i}^{i+B}} \\frac{\\partial\\ell_{s}}{\\partial w_j} = \\color{red}{\\sum_{s = i}^{i + B}} h_j(\\mathbf{x}_s)\\left(\\mathbf{1}[y_s = +1] - P(y_s = +1 | \\mathbf{x}_s, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "** Computing the gradient for a \"mini-batch\" of data points**\n",
    "\n",
    "Using NumPy, we access the points $\\mathbf{x}_i, \\mathbf{x}_{i+1} \\ldots \\mathbf{x}_{i+B}$ in the training data using `feature_matrix_train[i:i+B,:]`\n",
    "and $y_i$ in the training data using `sentiment_train[i:i+B]`. \n",
    "\n",
    "We can compute $\\color{red}{\\sum_{s = i}^{i+B}} \\partial\\ell_{s}/\\partial w_j$ easily as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient mini-batch data points: 1.0\n",
      "                --> Should print 1.0\n"
     ]
    }
   ],
   "source": [
    "j = 1                        # Feature number\n",
    "i = 10                       # Data point start\n",
    "B = 10                       # Mini-batch size\n",
    "coefficients = np.zeros(194) # A point w at which we are computing the gradient.\n",
    "\n",
    "predictions = predict_probability(feature_matrix_train[i:i+B,:], coefficients)\n",
    "indicator = (sentiment_train[i:i+B]==+1)\n",
    "\n",
    "errors = indicator - predictions        \n",
    "gradient_mini_batch = feature_derivative(errors, feature_matrix_train[i:i+B,j])\n",
    "print \"Gradient mini-batch data points: %s\" % gradient_mini_batch\n",
    "print \"                --> Should print 1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "** Quiz Question:** The code block above computed \n",
    "$\\color{red}{\\sum_{s = i}^{i+B}}\\partial\\ell_{s}(\\mathbf{w})/{\\partial w_j}$ \n",
    "for `j = 10`, `i = 10`, and `B = 10`. Is this a scalar or a 194-dimensional vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "This is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "** Quiz Question:** For what value of `B` is the term\n",
    "$\\color{red}{\\sum_{s = 1}^{B}}\\partial\\ell_{s}(\\mathbf{w})/\\partial w_j$\n",
    "the same as the full gradient\n",
    "$\\partial\\ell(\\mathbf{w})/{\\partial w_j}$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47780"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_matrix_train) # wrong (but seems like it should be right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_matrix_train[0,:]) # wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### Averaging the gradient across a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "It is a common practice to normalize the gradient update rule by the batch size B:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell_{\\color{red}{A}}(\\mathbf{w})}{\\partial w_j} \\approx \\color{red}{\\frac{1}{B}} {\\sum_{s = i}^{i + B}} h_j(\\mathbf{x}_s)\\left(\\mathbf{1}[y_s = +1] - P(y_s = +1 | \\mathbf{x}_s, \\mathbf{w})\\right)\n",
    "$$\n",
    "In other words, we update the coefficients using the **average gradient over data points** (instead of using a summation). By using the average gradient, we ensure that the magnitude of the gradient is approximately the same for all batch sizes. This way, we can more easily compare various batch sizes of stochastic gradient ascent (including a batch size of **all the data points**), and study the effect of batch size on the algorithm as well as the choice of step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Implementing stochastic gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Now we are ready to implement our own logistic regression with stochastic gradient ascent. Complete the following function to fit a logistic regression model using gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def logistic_regression_SG(feature_matrix, sentiment, initial_coefficients, step_size, batch_size, max_iter):\n",
    "    log_likelihood_all = []\n",
    "    \n",
    "    # make sure it's a numpy array\n",
    "    coefficients = np.array(initial_coefficients)\n",
    "    # set seed=1 to produce consistent results\n",
    "    np.random.seed(seed=1)\n",
    "    # Shuffle the data before starting\n",
    "    permutation = np.random.permutation(len(feature_matrix))\n",
    "    feature_matrix = feature_matrix[permutation,:]\n",
    "    sentiment = sentiment[permutation]\n",
    "    \n",
    "    i = 0 # index of current batch\n",
    "    # Do a linear scan over data\n",
    "    for itr in xrange(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        # Make sure to slice the i-th row of feature_matrix with [i:i+batch_size,:]\n",
    "        ### YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix[i: i+batch_size, :], coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        # Make sure to slice the i-th entry with [i:i+batch_size]\n",
    "        ### YOUR CODE HERE\n",
    "        indicator = (sentiment[i:i+batch_size] == 1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n",
    "            # Compute the derivative for coefficients[j] and save it to derivative.\n",
    "            # Make sure to slice the i-th row of feature_matrix with [i:i+batch_size,j]\n",
    "            ### YOUR CODE HERE\n",
    "            derivative = feature_derivative(errors, feature_matrix[i: i+batch_size, j])\n",
    "            \n",
    "            # compute the product of the step size, the derivative, and the **normalization constant** (1./batch_size)\n",
    "            ### YOUR CODE HERE\n",
    "            coefficients[j] += (step_size * derivative)/batch_size\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        # Print the log likelihood over the *current batch*\n",
    "        lp = compute_avg_log_likelihood(feature_matrix[i:i+batch_size,:], sentiment[i:i+batch_size],\n",
    "                                        coefficients)\n",
    "        log_likelihood_all.append(lp)\n",
    "        if itr <= 15 or (itr <= 1000 and itr % 100 == 0) or (itr <= 10000 and itr % 1000 == 0) \\\n",
    "         or itr % 10000 == 0 or itr == max_iter-1:\n",
    "            data_size = len(feature_matrix)\n",
    "            print 'Iteration %*d: Average log likelihood (of data points in batch [%0*d:%0*d]) = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, \\\n",
    "                 int(np.ceil(np.log10(data_size))), i, \\\n",
    "                 int(np.ceil(np.log10(data_size))), i+batch_size, lp)\n",
    "        \n",
    "        # if we made a complete pass over data, shuffle and restart\n",
    "        i += batch_size\n",
    "        if i+batch_size > len(feature_matrix):\n",
    "            permutation = np.random.permutation(len(feature_matrix))\n",
    "            feature_matrix = feature_matrix[permutation,:]\n",
    "            sentiment = sentiment[permutation]\n",
    "            i = 0\n",
    "                \n",
    "    # We return the list of log likelihoods for plotting purposes.\n",
    "    return coefficients, log_likelihood_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Note**. In practice, the final set of coefficients is rarely used; it is better to use the average of the last K sets of coefficients instead, where K should be adjusted depending on how fast the log likelihood oscillates around the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The following cell tests your stochastic gradient ascent function using a toy dataset consisting of two data points. If the test does not pass, make sure you are normalizing the gradient update rule correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Average log likelihood (of data points in batch [0:2]) = -0.33774513\n",
      "Iteration 1: Average log likelihood (of data points in batch [0:2]) = -0.23455309\n",
      "-------------------------------------------------------------------------------------\n",
      "Coefficients learned                 : [-0.09755757  0.68242552 -0.7799831 ]\n",
      "Average log likelihood per-iteration : [-0.33774513108142956, -0.2345530939410341]\n",
      "-------------------------------------------------------------------------------------\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "sample_feature_matrix = np.array([[1.,2.,-1.], [1.,0.,1.]])\n",
    "sample_sentiment = np.array([+1, -1])\n",
    "\n",
    "coefficients, log_likelihood = logistic_regression_SG(sample_feature_matrix, sample_sentiment, np.zeros(3),\n",
    "                                                  step_size=1., batch_size=2, max_iter=2)\n",
    "print '-------------------------------------------------------------------------------------'\n",
    "print 'Coefficients learned                 :', coefficients\n",
    "print 'Average log likelihood per-iteration :', log_likelihood\n",
    "if np.allclose(coefficients, np.array([-0.09755757,  0.68242552, -0.7799831]), atol=1e-3)\\\n",
    "  and np.allclose(log_likelihood, np.array([-0.33774513108142956, -0.2345530939410341])):\n",
    "    # pass if elements match within 1e-3\n",
    "    print '-------------------------------------------------------------------------------------'\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print '-------------------------------------------------------------------------------------'\n",
    "    print 'Test failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Compare convergence behavior of stochastic gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "For the remainder of the assignment, we will compare stochastic gradient ascent against batch gradient ascent. For this, we need a reference implementation of batch gradient ascent. But do we need to implement this from scratch?\n",
    "\n",
    "**Quiz Question:** For what value of batch size `B` above is the stochastic gradient ascent function **logistic_regression_SG** act as a standard gradient ascent algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "To the number of data points in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47780"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Running gradient ascent using the stochastic gradient ascent implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Instead of implementing batch gradient ascent separately, we save time by re-using the stochastic gradient ascent function we just wrote &mdash; **to perform gradient ascent**, it suffices to set **`batch_size`** to the number of data points in the training data. Yes, we did answer above the quiz question for you, but that is an important point to remember in the future :)\n",
    "\n",
    "**Small Caveat**. The batch gradient ascent implementation here is slightly different than the one in the earlier assignments, as we now normalize the gradient update rule.\n",
    "\n",
    "We now **run stochastic gradient ascent** over the **feature_matrix_train** for 10 iterations using:\n",
    "* `initial_coefficients = np.zeros(194)`\n",
    "* `step_size = 5e-1`\n",
    "* `batch_size = 1`\n",
    "* `max_iter = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Average log likelihood (of data points in batch [00000:00001]) = -0.25192908\n",
      "Iteration 1: Average log likelihood (of data points in batch [00001:00002]) = -0.00000001\n",
      "Iteration 2: Average log likelihood (of data points in batch [00002:00003]) = -0.12692771\n",
      "Iteration 3: Average log likelihood (of data points in batch [00003:00004]) = -0.02969101\n",
      "Iteration 4: Average log likelihood (of data points in batch [00004:00005]) = -0.02668819\n",
      "Iteration 5: Average log likelihood (of data points in batch [00005:00006]) = -0.04332901\n",
      "Iteration 6: Average log likelihood (of data points in batch [00006:00007]) = -0.02368802\n",
      "Iteration 7: Average log likelihood (of data points in batch [00007:00008]) = -0.12686897\n",
      "Iteration 8: Average log likelihood (of data points in batch [00008:00009]) = -0.04468879\n",
      "Iteration 9: Average log likelihood (of data points in batch [00009:00010]) = -0.00000124\n"
     ]
    }
   ],
   "source": [
    "coefficients, log_likelihood = logistic_regression_SG(feature_matrix_train, sentiment_train,\n",
    "                                        initial_coefficients=np.zeros(194),\n",
    "                                        step_size=5e-1, batch_size=1, max_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Quiz Question**. When you set `batch_size = 1`, as each iteration passes, how does the average log likelihood in the batch change?\n",
    "* Increases\n",
    "* Decreases\n",
    "* Fluctuates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "It Fluctuates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Now run **batch gradient ascent** over the **feature_matrix_train** for 200 iterations using:\n",
    "* `initial_coefficients = np.zeros(194)`\n",
    "* `step_size = 5e-1`\n",
    "* `batch_size = len(feature_matrix_train)`\n",
    "* `max_iter = 200`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 199: Average log likelihood (of data points in batch [00000:47780]) = -0.47126953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: Average log likelihood (of data points in batch [00000:47780]) = -0.49520194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  15: Average log likelihood (of data points in batch [00000:47780]) = -0.59602682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  14: Average log likelihood (of data points in batch [00000:47780]) = -0.59968811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  13: Average log likelihood (of data points in batch [00000:47780]) = -0.60354390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  12: Average log likelihood (of data points in batch [00000:47780]) = -0.60761103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  11: Average log likelihood (of data points in batch [00000:47780]) = -0.61190832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  10: Average log likelihood (of data points in batch [00000:47780]) = -0.61645691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   9: Average log likelihood (of data points in batch [00000:47780]) = -0.62128063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   8: Average log likelihood (of data points in batch [00000:47780]) = -0.62640636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   7: Average log likelihood (of data points in batch [00000:47780]) = -0.63186462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   6: Average log likelihood (of data points in batch [00000:47780]) = -0.63769009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   5: Average log likelihood (of data points in batch [00000:47780]) = -0.64392241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   4: Average log likelihood (of data points in batch [00000:47780]) = -0.65060701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   3: Average log likelihood (of data points in batch [00000:47780]) = -0.65779626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   2: Average log likelihood (of data points in batch [00000:47780]) = -0.66555129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   1: Average log likelihood (of data points in batch [00000:47780]) = -0.67394599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   0: Average log likelihood (of data points in batch [00000:47780]) = -0.68308119\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "coefficients_batch, log_likelihood_batch = logistic_regression_SG(feature_matrix_train, sentiment_train,\n",
    "                                                                  initial_coefficients=np.zeros(194), step_size=5e-1, batch_size=len(feature_matrix_train), max_iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Quiz Question**. When you set `batch_size = len(train_data)`, as each iteration passes, how does the average log likelihood in the batch change?\n",
    "* Increases \n",
    "* Decreases\n",
    "* Fluctuates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Make \"passes\" over the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "To make a fair comparison betweeen stochastic gradient ascent and batch gradient ascent, we measure the average log likelihood as a function of the number of passes (defined as follows):\n",
    "$$\n",
    "[\\text{# of passes}] = \\frac{[\\text{# of data points touched so far}]}{[\\text{size of dataset}]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Quiz Question** Suppose that we run stochastic gradient ascent with a batch size of 100. How many gradient updates are performed at the end of two passes over a dataset consisting of 50000 data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n",
      "1,000.0\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 2.\n",
    "size_of_dataset = 50000\n",
    "batch_size = 100\n",
    "\n",
    "one_pass = size_of_dataset/float(batch_size)\n",
    "two_passes = 2 * one_pass\n",
    "print('{0:,}'.format(number_of_passes * batch_size)) # marked wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "2 updates # marked wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000.0\n"
     ]
    }
   ],
   "source": [
    "print('{0:,}'.format(two_passes))  # this is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Log likelihood plots for stochastic gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "With the terminology in mind, let us run stochastic gradient ascent for 10 passes. We will use\n",
    "* `step_size=1e-1`\n",
    "* `batch_size=100`\n",
    "* `initial_coefficients` to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4769: Average log likelihood (of data points in batch [47600:47700]) = -0.54670667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Average log likelihood (of data points in batch [18400:18500]) = -0.51096892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Average log likelihood (of data points in batch [13800:13900]) = -0.36466901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Average log likelihood (of data points in batch [09200:09300]) = -0.46365531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Average log likelihood (of data points in batch [04600:04700]) = -0.46988191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  800: Average log likelihood (of data points in batch [32300:32400]) = -0.46963453\n",
      "Iteration  900: Average log likelihood (of data points in batch [42300:42400]) = -0.47883783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500: Average log likelihood (of data points in batch [02300:02400]) = -0.47223389\n",
      "Iteration  600: Average log likelihood (of data points in batch [12300:12400]) = -0.52216798\n",
      "Iteration  700: Average log likelihood (of data points in batch [22300:22400]) = -0.52336683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  300: Average log likelihood (of data points in batch [30000:30100]) = -0.48245740\n",
      "Iteration  400: Average log likelihood (of data points in batch [40000:40100]) = -0.46629313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Average log likelihood (of data points in batch [00000:00100]) = -0.68251093\n",
      "Iteration    1: Average log likelihood (of data points in batch [00100:00200]) = -0.67845294\n",
      "Iteration    2: Average log likelihood (of data points in batch [00200:00300]) = -0.68207160\n",
      "Iteration    3: Average log likelihood (of data points in batch [00300:00400]) = -0.67411325\n",
      "Iteration    4: Average log likelihood (of data points in batch [00400:00500]) = -0.67804438\n",
      "Iteration    5: Average log likelihood (of data points in batch [00500:00600]) = -0.67712546\n",
      "Iteration    6: Average log likelihood (of data points in batch [00600:00700]) = -0.66377074\n",
      "Iteration    7: Average log likelihood (of data points in batch [00700:00800]) = -0.67321231\n",
      "Iteration    8: Average log likelihood (of data points in batch [00800:00900]) = -0.66923613\n",
      "Iteration    9: Average log likelihood (of data points in batch [00900:01000]) = -0.67479446\n",
      "Iteration   10: Average log likelihood (of data points in batch [01000:01100]) = -0.66501639\n",
      "Iteration   11: Average log likelihood (of data points in batch [01100:01200]) = -0.65591964\n",
      "Iteration   12: Average log likelihood (of data points in batch [01200:01300]) = -0.66240398\n",
      "Iteration   13: Average log likelihood (of data points in batch [01300:01400]) = -0.66440641\n",
      "Iteration   14: Average log likelihood (of data points in batch [01400:01500]) = -0.65782757\n",
      "Iteration   15: Average log likelihood (of data points in batch [01500:01600]) = -0.64571479\n",
      "Iteration  100: Average log likelihood (of data points in batch [10000:10100]) = -0.60976663\n",
      "Iteration  200: Average log likelihood (of data points in batch [20000:20100]) = -0.54566060\n"
     ]
    }
   ],
   "source": [
    "step_size = 1e-1\n",
    "batch_size = 100\n",
    "num_passes = 10\n",
    "num_iterations = num_passes * int(len(feature_matrix_train)/float(batch_size))\n",
    "\n",
    "coefficients_sgd, log_likelihood_sgd = logistic_regression_SG(feature_matrix_train, sentiment_train,\n",
    "                                       initial_coefficients=np.zeros(194),\n",
    "                                       step_size=step_size, batch_size=batch_size, max_iter=num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "We provide you with a utility function to plot the average log likelihood as a function of the number of passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def make_plot(log_likelihood_all, len_data, batch_size, smoothing_window=1, label=''):\n",
    "    plt.rcParams.update({'figure.figsize': (9,5)})\n",
    "    log_likelihood_all_ma = np.convolve(np.array(log_likelihood_all), \\\n",
    "                                        np.ones((smoothing_window,))/smoothing_window, mode='valid')\n",
    "    plt.plot(np.array(range(smoothing_window-1, len(log_likelihood_all)))*float(batch_size)/len_data,\n",
    "             log_likelihood_all_ma, linewidth=4.0, label=label)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('# of passes over data')\n",
    "    plt.ylabel('Average log likelihood per data point')\n",
    "    plt.legend(loc='lower right', prop={'size':14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31f2bf2b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_plot(log_likelihood_sgd, len_data=len(feature_matrix_train), batch_size=100,\n",
    "          label='stochastic gradient, step_size=1e-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Smoothing the stochastic gradient ascent curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "The plotted line oscillates so much that it is hard to see whether the log likelihood is improving. In our plot, we apply a simple smoothing operation using the parameter `smoothing_window`. The smoothing is simply a [moving average](https://en.wikipedia.org/wiki/Moving_average) of log likelihood over the last `smoothing_window` \"iterations\" of  stochastic gradient ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31f21c9c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_plot(log_likelihood_sgd, len_data=len(feature_matrix_train), batch_size=100,\n",
    "          smoothing_window=30, label='stochastic gradient, step_size=1e-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31f2463450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_plot(log_likelihood_sgd, len_data=len(feature_matrix_train), batch_size=100,\n",
    "          smoothing_window=500, label='stochastic gradient, step_size=1e-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Checkpoint**: The above plot should look smoother than the previous plot. Play around with `smoothing_window`. As you increase it, you should see a smoother plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Stochastic gradient ascent vs batch gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "To compare convergence rates for stochastic gradient ascent with batch gradient ascent, we call `make_plot()` multiple times in the same cell.\n",
    "\n",
    "We are comparing:\n",
    "* **stochastic gradient ascent**: `step_size = 0.1`, `batch_size=100`\n",
    "* **batch gradient ascent**: `step_size = 0.5`, `batch_size=len(feature_matrix_train)`\n",
    "\n",
    "Write code to run stochastic gradient ascent for 200 passes using:\n",
    "* `step_size=1e-1`\n",
    "* `batch_size=100`\n",
    "* `initial_coefficients` to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 95399: Average log likelihood (of data points in batch [47600:47700]) = -0.50265709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90000: Average log likelihood (of data points in batch [32400:32500]) = -0.43220080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80000: Average log likelihood (of data points in batch [34100:34200]) = -0.45494095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70000: Average log likelihood (of data points in batch [35800:35900]) = -0.49905981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60000: Average log likelihood (of data points in batch [37500:37600]) = -0.41599369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50000: Average log likelihood (of data points in batch [39200:39300]) = -0.46859254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40000: Average log likelihood (of data points in batch [40900:41000]) = -0.45899229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30000: Average log likelihood (of data points in batch [42600:42700]) = -0.41913672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20000: Average log likelihood (of data points in batch [44300:44400]) = -0.48958438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000: Average log likelihood (of data points in batch [46000:46100]) = -0.45394262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  9000: Average log likelihood (of data points in batch [41400:41500]) = -0.45267388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  8000: Average log likelihood (of data points in batch [36800:36900]) = -0.39989352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  7000: Average log likelihood (of data points in batch [32200:32300]) = -0.42656766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  6000: Average log likelihood (of data points in batch [27600:27700]) = -0.45656653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  5000: Average log likelihood (of data points in batch [23000:23100]) = -0.43544394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  4000: Average log likelihood (of data points in batch [18400:18500]) = -0.51096892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  3000: Average log likelihood (of data points in batch [13800:13900]) = -0.36466901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  2000: Average log likelihood (of data points in batch [09200:09300]) = -0.46365531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1000: Average log likelihood (of data points in batch [04600:04700]) = -0.46988191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   800: Average log likelihood (of data points in batch [32300:32400]) = -0.46963453\n",
      "Iteration   900: Average log likelihood (of data points in batch [42300:42400]) = -0.47883783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   500: Average log likelihood (of data points in batch [02300:02400]) = -0.47223389\n",
      "Iteration   600: Average log likelihood (of data points in batch [12300:12400]) = -0.52216798\n",
      "Iteration   700: Average log likelihood (of data points in batch [22300:22400]) = -0.52336683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   300: Average log likelihood (of data points in batch [30000:30100]) = -0.48245740\n",
      "Iteration   400: Average log likelihood (of data points in batch [40000:40100]) = -0.46629313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     0: Average log likelihood (of data points in batch [00000:00100]) = -0.68251093\n",
      "Iteration     1: Average log likelihood (of data points in batch [00100:00200]) = -0.67845294\n",
      "Iteration     2: Average log likelihood (of data points in batch [00200:00300]) = -0.68207160\n",
      "Iteration     3: Average log likelihood (of data points in batch [00300:00400]) = -0.67411325\n",
      "Iteration     4: Average log likelihood (of data points in batch [00400:00500]) = -0.67804438\n",
      "Iteration     5: Average log likelihood (of data points in batch [00500:00600]) = -0.67712546\n",
      "Iteration     6: Average log likelihood (of data points in batch [00600:00700]) = -0.66377074\n",
      "Iteration     7: Average log likelihood (of data points in batch [00700:00800]) = -0.67321231\n",
      "Iteration     8: Average log likelihood (of data points in batch [00800:00900]) = -0.66923613\n",
      "Iteration     9: Average log likelihood (of data points in batch [00900:01000]) = -0.67479446\n",
      "Iteration    10: Average log likelihood (of data points in batch [01000:01100]) = -0.66501639\n",
      "Iteration    11: Average log likelihood (of data points in batch [01100:01200]) = -0.65591964\n",
      "Iteration    12: Average log likelihood (of data points in batch [01200:01300]) = -0.66240398\n",
      "Iteration    13: Average log likelihood (of data points in batch [01300:01400]) = -0.66440641\n",
      "Iteration    14: Average log likelihood (of data points in batch [01400:01500]) = -0.65782757\n",
      "Iteration    15: Average log likelihood (of data points in batch [01500:01600]) = -0.64571479\n",
      "Iteration   100: Average log likelihood (of data points in batch [10000:10100]) = -0.60976663\n",
      "Iteration   200: Average log likelihood (of data points in batch [20000:20100]) = -0.54566060\n"
     ]
    }
   ],
   "source": [
    "step_size = 1e-1\n",
    "batch_size = 100\n",
    "num_passes = 200\n",
    "num_iterations = num_passes * int(len(feature_matrix_train)/float(batch_size))\n",
    "\n",
    "## YOUR CODE HERE\n",
    "coefficients_sgd, log_likelihood_sgd = logistic_regression_SG(feature_matrix_train, sentiment_train,\n",
    "                                       initial_coefficients=np.zeros(194),\n",
    "                                       step_size=step_size, batch_size=batch_size, max_iter=num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "We compare the convergence of stochastic gradient ascent and batch gradient ascent in the following cell. Note that we apply smoothing with `smoothing_window=30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31f233d190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_plot(log_likelihood_sgd, len_data=len(feature_matrix_train), batch_size=100,\n",
    "          smoothing_window=30, label='stochastic, step_size=1e-1')\n",
    "make_plot(log_likelihood_batch, len_data=len(feature_matrix_train), batch_size=len(feature_matrix_train),\n",
    "          smoothing_window=1, label='batch, step_size=5e-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Quiz Question**: In the figure above, how many passes does batch gradient ascent need to achieve a similar log likelihood as stochastic gradient ascent? \n",
    "\n",
    "1. It's always better\n",
    "2. 10 passes\n",
    "3. 20 passes\n",
    "4. 150 passes or more\n",
    "\n",
    "1 it's always better # marked wrong\n",
    "\n",
    "4. 150 passes or more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "## Explore the effects of step sizes on stochastic gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "In previous sections, we chose step sizes for you. In practice, it helps to know how to choose good step sizes yourself.\n",
    "\n",
    "To start, we explore a wide range of step sizes that are equally spaced in the log space. Run stochastic gradient ascent with `step_size` set to 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, and 1e2. Use the following set of parameters:\n",
    "* `initial_coefficients=np.zeros(194)`\n",
    "* `batch_size=100`\n",
    "* `max_iter` initialized so as to run 10 passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4769: Average log likelihood (of data points in batch [47600:47700]) = -19.80362650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Average log likelihood (of data points in batch [18400:18500]) = -12.93654541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Average log likelihood (of data points in batch [13800:13900]) = -1.92461565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Average log likelihood (of data points in batch [09200:09300]) = -12.29692100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Average log likelihood (of data points in batch [04600:04700]) = -7.82375735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  800: Average log likelihood (of data points in batch [32300:32400]) = -6.89222536\n",
      "Iteration  900: Average log likelihood (of data points in batch [42300:42400]) = -6.27065449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500: Average log likelihood (of data points in batch [02300:02400]) = -12.30212504\n",
      "Iteration  600: Average log likelihood (of data points in batch [12300:12400]) = -13.20710239\n",
      "Iteration  700: Average log likelihood (of data points in batch [22300:22400]) = -9.11073480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  300: Average log likelihood (of data points in batch [30000:30100]) = -8.95363430\n",
      "Iteration  400: Average log likelihood (of data points in batch [40000:40100]) = -5.54126039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Average log likelihood (of data points in batch [00000:00100]) = -2.44471310\n",
      "Iteration    1: Average log likelihood (of data points in batch [00100:00200]) = -36.66862050\n",
      "Iteration    2: Average log likelihood (of data points in batch [00200:00300]) = -25.49870239\n",
      "Iteration    3: Average log likelihood (of data points in batch [00300:00400]) = -40.14565040\n",
      "Iteration    4: Average log likelihood (of data points in batch [00400:00500]) = -27.03748522\n",
      "Iteration    5: Average log likelihood (of data points in batch [00500:00600]) = -32.62294582\n",
      "Iteration    6: Average log likelihood (of data points in batch [00600:00700]) = -25.88017915\n",
      "Iteration    7: Average log likelihood (of data points in batch [00700:00800]) = -37.30720216\n",
      "Iteration    8: Average log likelihood (of data points in batch [00800:00900]) = -10.87360529\n",
      "Iteration    9: Average log likelihood (of data points in batch [00900:01000]) = -6.60878996\n",
      "Iteration   10: Average log likelihood (of data points in batch [01000:01100]) = -7.15375088\n",
      "Iteration   11: Average log likelihood (of data points in batch [01100:01200]) = -6.04741293\n",
      "Iteration   12: Average log likelihood (of data points in batch [01200:01300]) = -18.17389834\n",
      "Iteration   13: Average log likelihood (of data points in batch [01300:01400]) = -27.14619228\n",
      "Iteration   14: Average log likelihood (of data points in batch [01400:01500]) = -20.50685042\n",
      "Iteration   15: Average log likelihood (of data points in batch [01500:01600]) = -7.74332305\n",
      "Iteration  100: Average log likelihood (of data points in batch [10000:10100]) = -10.64501703\n",
      "Iteration  200: Average log likelihood (of data points in batch [20000:20100]) = -4.03459887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4769: Average log likelihood (of data points in batch [47600:47700]) = -2.48776279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Average log likelihood (of data points in batch [18400:18500]) = -2.47834692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Average log likelihood (of data points in batch [13800:13900]) = -0.29378688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Average log likelihood (of data points in batch [09200:09300]) = -0.45227508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Average log likelihood (of data points in batch [04600:04700]) = -0.47236476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  800: Average log likelihood (of data points in batch [32300:32400]) = -0.66616640\n",
      "Iteration  900: Average log likelihood (of data points in batch [42300:42400]) = -0.46114004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500: Average log likelihood (of data points in batch [02300:02400]) = -0.31220572\n",
      "Iteration  600: Average log likelihood (of data points in batch [12300:12400]) = -1.89468446\n",
      "Iteration  700: Average log likelihood (of data points in batch [22300:22400]) = -0.96096585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  300: Average log likelihood (of data points in batch [30000:30100]) = -0.72860037\n",
      "Iteration  400: Average log likelihood (of data points in batch [40000:40100]) = -0.58719556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Average log likelihood (of data points in batch [00000:00100]) = -0.51319004\n",
      "Iteration    1: Average log likelihood (of data points in batch [00100:00200]) = -2.20035379\n",
      "Iteration    2: Average log likelihood (of data points in batch [00200:00300]) = -3.34199720\n",
      "Iteration    3: Average log likelihood (of data points in batch [00300:00400]) = -3.06285156\n",
      "Iteration    4: Average log likelihood (of data points in batch [00400:00500]) = -2.80822162\n",
      "Iteration    5: Average log likelihood (of data points in batch [00500:00600]) = -2.99629286\n",
      "Iteration    6: Average log likelihood (of data points in batch [00600:00700]) = -2.71489944\n",
      "Iteration    7: Average log likelihood (of data points in batch [00700:00800]) = -3.61713200\n",
      "Iteration    8: Average log likelihood (of data points in batch [00800:00900]) = -1.19526584\n",
      "Iteration    9: Average log likelihood (of data points in batch [00900:01000]) = -0.75357081\n",
      "Iteration   10: Average log likelihood (of data points in batch [01000:01100]) = -0.71310829\n",
      "Iteration   11: Average log likelihood (of data points in batch [01100:01200]) = -0.59361318\n",
      "Iteration   12: Average log likelihood (of data points in batch [01200:01300]) = -1.53764659\n",
      "Iteration   13: Average log likelihood (of data points in batch [01300:01400]) = -2.69588686\n",
      "Iteration   14: Average log likelihood (of data points in batch [01400:01500]) = -1.89731473\n",
      "Iteration   15: Average log likelihood (of data points in batch [01500:01600]) = -0.81254441\n",
      "Iteration  100: Average log likelihood (of data points in batch [10000:10100]) = -1.19013437\n",
      "Iteration  200: Average log likelihood (of data points in batch [20000:20100]) = -0.48968363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4769: Average log likelihood (of data points in batch [47600:47700]) = -0.52452720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Average log likelihood (of data points in batch [18400:18500]) = -0.49162447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Average log likelihood (of data points in batch [13800:13900]) = -0.32703099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Average log likelihood (of data points in batch [09200:09300]) = -0.43029376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Average log likelihood (of data points in batch [04600:04700]) = -0.43169967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  800: Average log likelihood (of data points in batch [32300:32400]) = -0.45343350\n",
      "Iteration  900: Average log likelihood (of data points in batch [42300:42400]) = -0.43128394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500: Average log likelihood (of data points in batch [02300:02400]) = -0.40170047\n",
      "Iteration  600: Average log likelihood (of data points in batch [12300:12400]) = -0.45117863\n",
      "Iteration  700: Average log likelihood (of data points in batch [22300:22400]) = -0.46493371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  300: Average log likelihood (of data points in batch [30000:30100]) = -0.35064478\n",
      "Iteration  400: Average log likelihood (of data points in batch [40000:40100]) = -0.38344116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Average log likelihood (of data points in batch [00000:00100]) = -0.61201447\n",
      "Iteration    1: Average log likelihood (of data points in batch [00100:00200]) = -0.58843678\n",
      "Iteration    2: Average log likelihood (of data points in batch [00200:00300]) = -0.59771677\n",
      "Iteration    3: Average log likelihood (of data points in batch [00300:00400]) = -0.58770466\n",
      "Iteration    4: Average log likelihood (of data points in batch [00400:00500]) = -0.56939710\n",
      "Iteration    5: Average log likelihood (of data points in batch [00500:00600]) = -0.57554451\n",
      "Iteration    6: Average log likelihood (of data points in batch [00600:00700]) = -0.54068090\n",
      "Iteration    7: Average log likelihood (of data points in batch [00700:00800]) = -0.55212916\n",
      "Iteration    8: Average log likelihood (of data points in batch [00800:00900]) = -0.55311029\n",
      "Iteration    9: Average log likelihood (of data points in batch [00900:01000]) = -0.57672007\n",
      "Iteration   10: Average log likelihood (of data points in batch [01000:01100]) = -0.55455807\n",
      "Iteration   11: Average log likelihood (of data points in batch [01100:01200]) = -0.49771894\n",
      "Iteration   12: Average log likelihood (of data points in batch [01200:01300]) = -0.54708765\n",
      "Iteration   13: Average log likelihood (of data points in batch [01300:01400]) = -0.54286814\n",
      "Iteration   14: Average log likelihood (of data points in batch [01400:01500]) = -0.52361054\n",
      "Iteration   15: Average log likelihood (of data points in batch [01500:01600]) = -0.49731367\n",
      "Iteration  100: Average log likelihood (of data points in batch [10000:10100]) = -0.50102061\n",
      "Iteration  200: Average log likelihood (of data points in batch [20000:20100]) = -0.42406927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4769: Average log likelihood (of data points in batch [47600:47700]) = -0.54670667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Average log likelihood (of data points in batch [18400:18500]) = -0.51096892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Average log likelihood (of data points in batch [13800:13900]) = -0.36466901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Average log likelihood (of data points in batch [09200:09300]) = -0.46365531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Average log likelihood (of data points in batch [04600:04700]) = -0.46988191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  800: Average log likelihood (of data points in batch [32300:32400]) = -0.46963453\n",
      "Iteration  900: Average log likelihood (of data points in batch [42300:42400]) = -0.47883783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500: Average log likelihood (of data points in batch [02300:02400]) = -0.47223389\n",
      "Iteration  600: Average log likelihood (of data points in batch [12300:12400]) = -0.52216798\n",
      "Iteration  700: Average log likelihood (of data points in batch [22300:22400]) = -0.52336683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  300: Average log likelihood (of data points in batch [30000:30100]) = -0.48245740\n",
      "Iteration  400: Average log likelihood (of data points in batch [40000:40100]) = -0.46629313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Average log likelihood (of data points in batch [00000:00100]) = -0.68251093\n",
      "Iteration    1: Average log likelihood (of data points in batch [00100:00200]) = -0.67845294\n",
      "Iteration    2: Average log likelihood (of data points in batch [00200:00300]) = -0.68207160\n",
      "Iteration    3: Average log likelihood (of data points in batch [00300:00400]) = -0.67411325\n",
      "Iteration    4: Average log likelihood (of data points in batch [00400:00500]) = -0.67804438\n",
      "Iteration    5: Average log likelihood (of data points in batch [00500:00600]) = -0.67712546\n",
      "Iteration    6: Average log likelihood (of data points in batch [00600:00700]) = -0.66377074\n",
      "Iteration    7: Average log likelihood (of data points in batch [00700:00800]) = -0.67321231\n",
      "Iteration    8: Average log likelihood (of data points in batch [00800:00900]) = -0.66923613\n",
      "Iteration    9: Average log likelihood (of data points in batch [00900:01000]) = -0.67479446\n",
      "Iteration   10: Average log likelihood (of data points in batch [01000:01100]) = -0.66501639\n",
      "Iteration   11: Average log likelihood (of data points in batch [01100:01200]) = -0.65591964\n",
      "Iteration   12: Average log likelihood (of data points in batch [01200:01300]) = -0.66240398\n",
      "Iteration   13: Average log likelihood (of data points in batch [01300:01400]) = -0.66440641\n",
      "Iteration   14: Average log likelihood (of data points in batch [01400:01500]) = -0.65782757\n",
      "Iteration   15: Average log likelihood (of data points in batch [01500:01600]) = -0.64571479\n",
      "Iteration  100: Average log likelihood (of data points in batch [10000:10100]) = -0.60976663\n",
      "Iteration  200: Average log likelihood (of data points in batch [20000:20100]) = -0.54566060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4769: Average log likelihood (of data points in batch [47600:47700]) = -0.56306510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Average log likelihood (of data points in batch [18400:18500]) = -0.54362587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Average log likelihood (of data points in batch [13800:13900]) = -0.45761063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Average log likelihood (of data points in batch [09200:09300]) = -0.54480104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Average log likelihood (of data points in batch [04600:04700]) = -0.59076267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  800: Average log likelihood (of data points in batch [32300:32400]) = -0.59156014\n",
      "Iteration  900: Average log likelihood (of data points in batch [42300:42400]) = -0.58842264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500: Average log likelihood (of data points in batch [02300:02400]) = -0.61150928\n",
      "Iteration  600: Average log likelihood (of data points in batch [12300:12400]) = -0.62979300\n",
      "Iteration  700: Average log likelihood (of data points in batch [22300:22400]) = -0.61553432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  300: Average log likelihood (of data points in batch [30000:30100]) = -0.63235099\n",
      "Iteration  400: Average log likelihood (of data points in batch [40000:40100]) = -0.62183600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Average log likelihood (of data points in batch [00000:00100]) = -0.69205420\n",
      "Iteration    1: Average log likelihood (of data points in batch [00100:00200]) = -0.69160695\n",
      "Iteration    2: Average log likelihood (of data points in batch [00200:00300]) = -0.69201686\n",
      "Iteration    3: Average log likelihood (of data points in batch [00300:00400]) = -0.69095428\n",
      "Iteration    4: Average log likelihood (of data points in batch [00400:00500]) = -0.69159348\n",
      "Iteration    5: Average log likelihood (of data points in batch [00500:00600]) = -0.69154386\n",
      "Iteration    6: Average log likelihood (of data points in batch [00600:00700]) = -0.68964000\n",
      "Iteration    7: Average log likelihood (of data points in batch [00700:00800]) = -0.69112685\n",
      "Iteration    8: Average log likelihood (of data points in batch [00800:00900]) = -0.69056997\n",
      "Iteration    9: Average log likelihood (of data points in batch [00900:01000]) = -0.69124730\n",
      "Iteration   10: Average log likelihood (of data points in batch [01000:01100]) = -0.68980179\n",
      "Iteration   11: Average log likelihood (of data points in batch [01100:01200]) = -0.68882576\n",
      "Iteration   12: Average log likelihood (of data points in batch [01200:01300]) = -0.68929536\n",
      "Iteration   13: Average log likelihood (of data points in batch [01300:01400]) = -0.69003572\n",
      "Iteration   14: Average log likelihood (of data points in batch [01400:01500]) = -0.68929307\n",
      "Iteration   15: Average log likelihood (of data points in batch [01500:01600]) = -0.68702353\n",
      "Iteration  100: Average log likelihood (of data points in batch [10000:10100]) = -0.67916061\n",
      "Iteration  200: Average log likelihood (of data points in batch [20000:20100]) = -0.66049079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4769: Average log likelihood (of data points in batch [47600:47700]) = -0.64571292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Average log likelihood (of data points in batch [18400:18500]) = -0.64157978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Average log likelihood (of data points in batch [13800:13900]) = -0.62798175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Average log likelihood (of data points in batch [09200:09300]) = -0.66156206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Average log likelihood (of data points in batch [04600:04700]) = -0.67367588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  800: Average log likelihood (of data points in batch [32300:32400]) = -0.67693088\n",
      "Iteration  900: Average log likelihood (of data points in batch [42300:42400]) = -0.67561867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500: Average log likelihood (of data points in batch [02300:02400]) = -0.68114554\n",
      "Iteration  600: Average log likelihood (of data points in batch [12300:12400]) = -0.68489867\n",
      "Iteration  700: Average log likelihood (of data points in batch [22300:22400]) = -0.68027821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  300: Average log likelihood (of data points in batch [30000:30100]) = -0.68492418\n",
      "Iteration  400: Average log likelihood (of data points in batch [40000:40100]) = -0.68415366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Average log likelihood (of data points in batch [00000:00100]) = -0.69303759\n",
      "Iteration    1: Average log likelihood (of data points in batch [00100:00200]) = -0.69299241\n",
      "Iteration    2: Average log likelihood (of data points in batch [00200:00300]) = -0.69303389\n",
      "Iteration    3: Average log likelihood (of data points in batch [00300:00400]) = -0.69292442\n",
      "Iteration    4: Average log likelihood (of data points in batch [00400:00500]) = -0.69299113\n",
      "Iteration    5: Average log likelihood (of data points in batch [00500:00600]) = -0.69298668\n",
      "Iteration    6: Average log likelihood (of data points in batch [00600:00700]) = -0.69278828\n",
      "Iteration    7: Average log likelihood (of data points in batch [00700:00800]) = -0.69294460\n",
      "Iteration    8: Average log likelihood (of data points in batch [00800:00900]) = -0.69288708\n",
      "Iteration    9: Average log likelihood (of data points in batch [00900:01000]) = -0.69295651\n",
      "Iteration   10: Average log likelihood (of data points in batch [01000:01100]) = -0.69280480\n",
      "Iteration   11: Average log likelihood (of data points in batch [01100:01200]) = -0.69270635\n",
      "Iteration   12: Average log likelihood (of data points in batch [01200:01300]) = -0.69274924\n",
      "Iteration   13: Average log likelihood (of data points in batch [01300:01400]) = -0.69283249\n",
      "Iteration   14: Average log likelihood (of data points in batch [01400:01500]) = -0.69275924\n",
      "Iteration   15: Average log likelihood (of data points in batch [01500:01600]) = -0.69251197\n",
      "Iteration  100: Average log likelihood (of data points in batch [10000:10100]) = -0.69158805\n",
      "Iteration  200: Average log likelihood (of data points in batch [20000:20100]) = -0.68946852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4769: Average log likelihood (of data points in batch [47600:47700]) = -0.68736824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000: Average log likelihood (of data points in batch [18400:18500]) = -0.68597545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000: Average log likelihood (of data points in batch [13800:13900]) = -0.68569701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000: Average log likelihood (of data points in batch [09200:09300]) = -0.68976850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Average log likelihood (of data points in batch [04600:04700]) = -0.69088883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  800: Average log likelihood (of data points in batch [32300:32400]) = -0.69139955\n",
      "Iteration  900: Average log likelihood (of data points in batch [42300:42400]) = -0.69123818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  500: Average log likelihood (of data points in batch [02300:02400]) = -0.69186710\n",
      "Iteration  600: Average log likelihood (of data points in batch [12300:12400]) = -0.69230650\n",
      "Iteration  700: Average log likelihood (of data points in batch [22300:22400]) = -0.69174220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  300: Average log likelihood (of data points in batch [30000:30100]) = -0.69228764\n",
      "Iteration  400: Average log likelihood (of data points in batch [40000:40100]) = -0.69222554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Average log likelihood (of data points in batch [00000:00100]) = -0.69313622\n",
      "Iteration    1: Average log likelihood (of data points in batch [00100:00200]) = -0.69313170\n",
      "Iteration    2: Average log likelihood (of data points in batch [00200:00300]) = -0.69313585\n",
      "Iteration    3: Average log likelihood (of data points in batch [00300:00400]) = -0.69312487\n",
      "Iteration    4: Average log likelihood (of data points in batch [00400:00500]) = -0.69313157\n",
      "Iteration    5: Average log likelihood (of data points in batch [00500:00600]) = -0.69313113\n",
      "Iteration    6: Average log likelihood (of data points in batch [00600:00700]) = -0.69311121\n",
      "Iteration    7: Average log likelihood (of data points in batch [00700:00800]) = -0.69312692\n",
      "Iteration    8: Average log likelihood (of data points in batch [00800:00900]) = -0.69312115\n",
      "Iteration    9: Average log likelihood (of data points in batch [00900:01000]) = -0.69312811\n",
      "Iteration   10: Average log likelihood (of data points in batch [01000:01100]) = -0.69311286\n",
      "Iteration   11: Average log likelihood (of data points in batch [01100:01200]) = -0.69310301\n",
      "Iteration   12: Average log likelihood (of data points in batch [01200:01300]) = -0.69310725\n",
      "Iteration   13: Average log likelihood (of data points in batch [01300:01400]) = -0.69311567\n",
      "Iteration   14: Average log likelihood (of data points in batch [01400:01500]) = -0.69310836\n",
      "Iteration   15: Average log likelihood (of data points in batch [01500:01600]) = -0.69308342\n",
      "Iteration  100: Average log likelihood (of data points in batch [10000:10100]) = -0.69298918\n",
      "Iteration  200: Average log likelihood (of data points in batch [20000:20100]) = -0.69277472\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_passes = 10\n",
    "num_iterations = num_passes * int(len(feature_matrix_train)/float(batch_size))\n",
    "\n",
    "coefficients_sgd = {}\n",
    "log_likelihood_sgd = {}\n",
    "for step_size in np.logspace(-4, 2, num=7):\n",
    "    coefficients_sgd[step_size], log_likelihood_sgd[step_size] = logistic_regression_SG(feature_matrix_train, sentiment_train,\n",
    "                                       initial_coefficients=np.zeros(194),\n",
    "                                       step_size=step_size, batch_size=batch_size, max_iter=num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "### Plotting the log likelihood as a function of passes for each step size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Now, we will plot the change in log likelihood using the `make_plot` for each of the following values of `step_size`:\n",
    "\n",
    "* `step_size = 1e-4`\n",
    "* `step_size = 1e-3`\n",
    "* `step_size = 1e-2`\n",
    "* `step_size = 1e-1`\n",
    "* `step_size = 1e0`\n",
    "* `step_size = 1e1`\n",
    "* `step_size = 1e2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "For consistency, we again apply `smoothing_window=30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31f2012150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step_size in np.logspace(-4, 2, num=7):\n",
    "    make_plot(log_likelihood_sgd[step_size], len_data=len(train_data), batch_size=100,\n",
    "              smoothing_window=30, label='step_size=%.1e'%step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Now, let us remove the step size `step_size = 1e2` and plot the rest of the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31f217eb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step_size in np.logspace(-4, 2, num=7)[0:6]:\n",
    "    make_plot(log_likelihood_sgd[step_size], len_data=len(train_data), batch_size=100,\n",
    "              smoothing_window=30, label='step_size=%.1e'%step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Quiz Question**: Which of the following is the worst step size? Pick the step size that results in the lowest log likelihood in the end.\n",
    "1. 1e-2\n",
    "2. 1e-1\n",
    "3. 1e0\n",
    "4. 1e1\n",
    "5. 1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "5. 1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "**Quiz Question**: Which of the following is the best step size? Pick the step size that results in the highest log likelihood in the end.\n",
    "1. 1e-4\n",
    "2. 1e-2\n",
    "3. 1e0\n",
    "4. 1e1\n",
    "5. 1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "4. 1e0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "module-10-online-learning-assignment-blank.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
